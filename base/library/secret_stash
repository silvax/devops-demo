#!/usr/bin/env python
'''
Created on Apr 30, 2015

@author: ryanbachman
'''


DOCUMENTATION = '''
---
module: secret_stash
short_description: Using kms for client side decryption
description:
    - Use a kms key to decrypt file from local disk or s3 and store in either location.
version_added: "1.0"
options:
    src:
        description:
            - SecretStash Name
        required: true
        default: null
    group:
        description:
            - SecretStash Group
        required: true
        default: null
    env:
        description:
            - Environment
        required: true
        default: null
    dest:
        description:
            - dest path of object
        required: false
        default: null
    format:
        description:
            - file format to parse
        required: false
        choices: ['json','yaml', None]
        default: None
    owner:
	      description:
	        - set ownership of kms saved file
	      required: false
    mode:
        description:
            - Change mode of saved file
        required: false

requirements: [ "boto 2.38+" ]

'''

EXAMPLES = '''
# Reads encrypted object from s3 path and stores it to destination file
- secret_stash: src=foo.crt group=GroupName env=dev dest=/etc/certs/foo.crt
# Parses s3 file as json and returns dictionary to be used in registered ansible variable
#   Values will be w/in the 'content' key of the returned dictionary
- secret_stash: src=foo.json group=another_group env=test format=json
# Set file owner to ec2-user and change file permissions
- secret_stash: src=foo.crt group=lastgroup env=stage dest=/etc/certs/foo.crt owner=ec2-user mode=0400
'''

import json
import yaml
import re
import os
import grp
import pwd
import base64
import requests
from Crypto.Cipher import AES
from ansible.module_utils.basic import *


try:
    from boto import kms as k
    from boto import s3 as s
except ImportError:
    kms_imported = False
else:
    kms_imported = True

try:
  r = requests.get('http://169.254.169.254/latest/dynamic/instance-identity/document', timeout=1)
  AWS_REGION = r.json()['region']
except requests.ConnectionError:
  AWS_REGION='us-east-1'

BUCKET_MAP = {
  'us-east-1': 'inin-kms-secured-data',
  'ap-southeast-2': 'inin-kms-secured-data-apse2'
}

S3_CONN = s.connect_to_region(AWS_REGION)

def decrypt(cipher_data):
    decode = lambda s : base64.b64decode(s)
    unpad = lambda s : s[0:-ord(s[-1])]
    context = cipher_data['context']
    iv = decode(cipher_data['iv'])
    KMS_CONN = k.connect_to_region(context['region'])
    decryption_key = KMS_CONN.decrypt(decode(cipher_data['encoded_key']), encryption_context=context)['Plaintext']
    key = AES.new(decryption_key, AES.MODE_CBC, iv)
    return unpad(key.decrypt(decode(cipher_data['content'])))

def parse(content,format):
    parse_opts = {
        'json' : parse_json,
        'yaml' : parse_yaml
    }
    if format is not None:
        return parse_opts[format](content)
    else:
        return content

def parse_json(content):
    return json.loads(content)

def parse_yaml(content):
    y = yaml.load(content)
    d = {}
    for k,v in y.iteritems():
        d[k] = v
    return d

def get_from_s3(**params):
    b = S3_CONN.get_bucket(params['bucket'], validate=False)
    key = b.get_key(params['key'])
    context = {}
    context['group'] = key.get_metadata('group')
    context['path'] = key.name
    context['region'] = key.get_metadata('region')
    context['account'] = key.get_metadata('account')
    return {'content': key.get_contents_as_string(), 'encoded_key': key.get_metadata('key'), 'iv' : key.get_metadata('iv'), 'context': context }

def write_to_file(content,filename,owner,perms):
    with open(filename, 'w+:') as f:
        os.chmod(filename, int(0200))
        f.write(content)
    try:
        os.chown(filename, pwd.getpwnam(owner).pw_uid, grp.getgrnam(owner).gr_gid)
        print "Setting Perms: {}".format(perms)
        if perms is not None: os.chmod(filename,int(perms, 8))
        return True
    except Exception, e:
        os.remove(filename)
        return False

def main():

    module = AnsibleModule(
        argument_spec = dict(
            src = dict(required=True),
            group = dict(required=True),
            env = dict(required=True),
            dest = dict(required=False,default=None),
            format = dict(required=False, default=None, choices=['json','yaml',None]),
            owner = dict(required=False, default='root'),
            mode = dict(required=False, default='400')
        )
    )

    if 'src' not in module.params:
      module.fail_json(msg='src parameter is required')
    if 'group' not in module.params:
      module.fail_json(msg='group parameter is required')
    if not kms_imported:
        module.fail_json(msg='Boto 2.38+ must be installed')
    opts = {}

    opts['bucket'] = BUCKET_MAP[AWS_REGION]
    opts['key'] = 'secret_stash/{}/{}/{}'.format(module.params['env'], module.params['group'], module.params['src'])
    try:
        content = str(decrypt(get_from_s3(bucket=opts['bucket'],key=opts['key'])))
        if module.params['dest'] is not None:
            if not write_to_file(content,module.params['dest'],module.params['owner'],module.params['mode']):
                module.fail_json(msg="Unable to change ownership to {}".format(module.params['owner']))
        if module.params['format'] is not None:
            parsed_data = parse(content,module.params['format'])
            #addition of sensitive flag disables logging of data contained in 'content' data in sumo_logs plugin
            module.exit_json(changed=True,content=parsed_data, sensitive=True)
        else:
            #addition of sensitive flag disables logging of data contained in 'content' data in sumo_logs plugin
            module.exit_json(changed=True,content=content, sensitive=True)
    except Exception, e:
        module.fail_json(msg=str(e))


if __name__ == '__main__':
    main()
